#+TITLE: Attributed Text Generation via Post-hoc Research and Revision
#+STARTUP: latexpreview
#+STARTUP: inlineimages
# Abstract
- LM's excel at many tasks, however, they do not have an attribution mechanism for external evidence.
- RARR provides a methodology of taking in LM outputs and revising the outputs to support attribution for external evidence.
- This is done by finding attribution for the LM's output, and editing the output to fix unsupported content. While also perserving the original output.
- RARR tends to improve the attribution while also keeping the output text faithful to the model output.
- RARR only requires few training samples, a LLM, and a web search.
# Introduction
- LM's and other TGMs are important for many AI systems.
- While LLM's are good at a variety of tasks there are still problems caused by the lack of attribution to evidence. This causes LLM's to generate false/biased/misleading statements ("hallucinations").
- We would like to trust the output of a TGM and provide an attribution report to justify the output.
- Most TGMs lack an attribution mechanism.
- Recent work has been done on training retrieval augments models (RAMs). These models retrieve a document and then generate text using the document as a condition for the output.
- While this improves performance it does not guarentee attribution since model's can optimize loss without having a high attribution rating.
- Therefore, LM and attribution are not necesarilly aligned when training RAMs.
- The high level idea for RARR is that we first generate a text prompt from a TGM, and then find attributing evidence.
- Afterwards we revise the text prompt to make it consistent with the evidence. Which helps to perserve the model's stylistic output.
- While TGMs struggle with factual memorization they are good at generating human sounding language. RARR acts to improve this retroactively by editing the output for factual soundness.
- This paper proposes new metrics, benchmarks, and modeling methods.
- The new metrics focus on preserving language while being factually sound
- Benchmarks focus on how editors work on the generated data
- Modeling techniques focus on improving the generalizability of the model to unseen domains using few-shot learning and LLMs.
# Task Formulation
- Given an input text x the model's goal is to generate the outputs y,A
- Where y is the output revision text and A is a list of evidence \({e_1,...,e_n}\) where the \(e_1\) is the retrieved documents for a given attribution report.
- We evaluate the output through two metrics.
- Attribution - How good the model utilizes the retrieved evidence.
- Preservation - How close y is to x.
- Attribution depends on both the quality of A to revise x and how the model modifies x to get y.
## Measuring Attribution
- Attribution is based on if given evidence from a set A that "According to A, y".
- The key thing to note is attribution is a binary evaluation.
- We further extend this to compute the average attribution over each sentence.
- Thus Attr_AIS(y,A) is a measure of the percentage of attribution.
- When establishing Attr we can also give surrounding sentences and context.
- When running evaluation we give a maximum number of evidence snippets in this case M = 5.
- To approximate human evaluation we use E2E NLI we call this model auto AIS. This model find incorrect sequences in a statement given factual information.
- Before running auto AIS we decontextualize each sentence.
- Decontextualization means that we merge multiple sentences/evidence and edit it to remove the need for context.
## Measuring Preservation
- To measure preservation we ask annotators to decide if the revion preserves the orginal text's message.
- This notion of message preservation is also a binary evaluation.
- We combine this with max(1-(Lev(x,y)/len(x)), 0) to have a metric which makes sure that intent is preserved with the minimal amount of edits.
- e.g. \(Pres_comb(x, y) = Pres_intent(x, y) * Pres_lev(x, y)\)
## Discussion
- We cannot optimize for attribution alone due to the fact that an adversary could replace x with \(y \in A\) and have a 100% attribution.
- Therefore we report a harmonic mean similar e.g. an F1_AP.
# Approach
- RARR is a system where given an input x it first generates a set of queries \({q_1, ..., q_n}\).
- This system then queries the internet for pieces of evidence snippets \({q_{11}, ..., q_{1n}, ..., q_{nn}}\).
- Finally we output all query evidence snippet pairs and use this for revision.
- Many components of RARR are developed using few-shot prompting of a LLM (in-context) learning.
- For this case we use PaLM.
- Developing these few shot prompts is really fast since it requires no model training.
## Reserach
- Research stages contains two steps
  1. query generation
  2. IR
### Query Generation
- In question generation, questions are the individual unites of information.
- Since it is natural language based it is both interpretable and highly flexible
