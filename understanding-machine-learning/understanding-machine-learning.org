#+TITLE:Understanding Machine Learning From Theory to Algorithms by Shai Shalev-Schwartz and Shai Ben-David
#+STARTUP: latexpreview

* 1. Introduction
** 1.1 What is learning
- Learning is the idea of turning experience into expertise.
- An example of this is bait shyness from rats. Where rats will taste food and if they are nauseous will be wary of the food.
- E.g. they label the certain food as poisonous.
- For email spam classification the naive "learning" approach can be seen in the rats where we just memorize what is spam and if it has been seen before label it as spam/not spam.
- While learning by memorization can be useful it lacks the ability of rats to generalize (inductive reasoning/inference)
- However, just inductive reasoning by itself can be bad since like Pigeon Superstition we can draw inferences that are only caused by correlation and not causation.
- Therefore, to prevent this type of learning in our ML systems we must provide clear definitions to prevent these "superstitions" from happening
- It has been shown that for rats they do not learn that electricical shocks or other negative effects from the food cause bait shyness.
- This is because there is a prior belief that the rats have that they need to learn from nausea.
- Thus we must also be able to include prior knowledge or an inductive bias to our machine learning systems so that they can learn well.
- The benefit of prior knowledge is that with more prior knowledge we can create a better learning algorithm which learns easier.
- However, this makes the learning algorithm less flexible.
** 1.2 When do we need ML
- We need ML for tasks that are too complex to program. (e.g. stuff that humans are able to do intuitively without precise definitions)
- Or tasks beyond human capabilities (e.g. analyzing big data)
- When we need adaptive systems due to changing inputs and outputs
** 1.3 Types of learning
- Supervised vs Unsupervised
- In the supervised case the learner receives some extra information such as labels
- This makes the learning task easier, and the main goal is to take the provided extra knowledge to apply it to unseen examples
- This is different from the unsupervised case where the main goal is to come up with some patterns that naturally exist within the dat
- There is also the middle case where we are provided some information but want to predict more information than is given.
- This is usually what is called reinforcement learning where we interact with an environment to get some small amounts of information (reward), and then try and predict more information about the given data (value of a chess board).
- Active vs Passive learning
- Active learning requires interaction with the environment (e.g. the model querys the environment for information)
- this differs from passive learning where the model is provided the labeled information ready to be digested.
- Helpfullness of the teacher
- The teacher (environment/data) can have different models of helpfullness
- Some teachers can be assumed as good teachers (clean data)
- While others can be malicious (adversarial learning)
- Online vs Batch
- Whether the model has to make predictions while still learning vs batch where information is fed in all at once.
** 1.4 Relations to other fields.
- ML has a lot of overal with statistics, information theory, game theory, and optimization
- ML is naturally a subfield of computer science but is extremely interdisciplinary.
- In contrast to traditional AI, ML is not as focused on creating automated intelligent behavior, rather using the strengths of computing to aid humans.
- Generally we assume that data is randomly generated, and our goal is to learn from this randomly generated data.
- Thus there is a lot of overlap with statistics.
- One main difference is that statistics is focused on hypothesis testing, where as machine learning is more focused on trying to find explanations using data.
- Another main difference is that algorithmic efficiency matters more in the machine learning setting.
- Finally, in statistics we assume many different things about the data distribution function (e.g. we are sampling from a random gaussian)
- Whereas in machine learning we try to work in setting that is distribution-free (e.g. we do not make assumptions about the distribution of the data)
* 2. A Gentle Start
- Initially we pose the learning task as with a simple problem. Assume that you arrive at a new island, and want to classify papayas. Given a set of features can we learn to classify papayas as tasty or not tasty
** 2.1 A Formal Model - The statistical learning framework
- The domain set is the set \(X\) is the set of all features that we wish to label. we refer to points in the domain set as instances
- The label set \(Y\) is the set of all labels for the case of this book we will usually assume that it is \(\{1, -1\}/\{0, 1\}\)
- The training data \(S\) is a sequence of points in \(X \times Y\) this is what the learner's input is
- The learner outputs a classifier \(h: X \rightarrow Y\). This is called a predictor/hypothesis/classifier.
- We designate a learning algorithm A and say that A(S) is the classifier the learning algorithm outputs.
- For a simple data generating model we assume that there is some underlying distribution that generates the data.
- We refer to this probability distribution as \(D\) and the distribution over some input set as \(D(X)\)
- We make little to no assumptions about the data generating distribution as to keep our analysis in a general sense.
- We assume there is some correct labeling function \(f:X \rightarrow Y\) where this function will correctly output \(y_i = f(x_i)\) \(\forall i\)
- We can say that each data point could be genereated by sampling some instance in the domain set, and then labeling it with the correct labeling function (true classifier)
- To measure success we look at some event A where \(A \subset X\). We denote \(D(A)\) as the probability of getting this event. Given some function \(\pi: X \rightarrow \{0, 1\}\) we can construct A such that A is the subset of X where the function outputs a 1. Finally we can denote the probability as \(P_{x \sim D}[\pi(x)]\)
- Thus we can formally define the measure of success as the probability of \(L_{(D, f)} = P_{x \sim D}[f(x) \neq h(x)] = D({x : f(x) \neq h(x)\)
- As a note the learner has no information over the data distribution D, and can only learn about the distribution through the training set \(S\)
** 2.2 Empirical Risk Minimization
- The output of a learner is \(h_s: X \rightarrow Y\) (a predictor)
- The goal is to find the predictor which minimizes the loss over the distribution and true classifier
- Since the learning algorithm cannot see the distribution we must minimize the Loss in respect to our training set
- We shall instead use the notion of the empircal risk/emprical error. Which is just the application of the true loss to a training set
- Formally defined as \(\frac{|\{x \in [m]: h(x_i) \neq f(x_i)\}|}{m}\)
- Coming up with a predictor that solves this is called the Empirical Risk Minimization
- The main problem with this is overfitting which is the case where the Empirical Risk is minimized but the True Risk is high
- Hence we need to find cases where ERM is solved while the True Risk is also low.
** 2.3 Empirical Risk with an Inductive Bias
- One way to solve this is to apply ERM to a restricted search space by choosing a set or predictors.
- This is solved by performing ERM over a hypothesis class called \(H\) where we try to solve \(ERM_H(S) \in argmin_{h \in H} L_S(h)\)
- By restricting this search space we are "biasing" the algorithm towards certain classifiers. This is known as an inductive bias.
- This bias should be based on some prior knowledge, however, Choosing a more restrictive hypothesis class will cause a stronger inductive bias. However, it will prevent the algorithm from overfitting.
- The simplest way we can restrict an algorithm is by restricting the size to a finite class.
- We can now prove that given a finite class it is always possible to solve the learning problem given a large enough sample size.
- We first define \(h_S \in argmin L_s(h)\)
- Next we assume that all points are sampled i.i.d.
- We also assume the realizability assumption where there will always exist a classifier that is perfect.
- Now because there is randomness in sampling our data distribution we must try and upperbound the probability of getting a bad training set, also we must assume that our answer cannot be correct due to the fact that our sample set may not cover the fine details.
- Hence we introduce parameters \(\delta\) to represent the probability of failure of learning and \(\epsilon\) to represent how accurate our predictor is (e.g. probability of being correct 1-risk)
- *Insert proof which upperbounds the probability of getting an unrepresentative/missleading sample is upperbounded by the value*
- \(D(S_{|x}: L(h) > \epsilon) \leq |H|e^{-\epsilon m}\)
- Thus solving for m we get the sample complexity to be \(m \geq \frac{log(|H|/\delta)}{\epsilon}\)
